{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53c79f174b1951b9194c9b2fad074676",
     "grade": false,
     "grade_id": "cell-c02122563d9a6938",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# The Titanic: A Tale of Tragedy and Data\n",
    "\n",
    "Over a century ago, in 1912, the RMS Titanic embarked on its maiden voyage, holding the hopes and dreams of many aboard. Hailed as the \"unsinkable\" ship, its tragic sinking after colliding with an iceberg remains one of the most infamous maritime disasters in history. Of the 2,224 passengers and crew members aboard, over 1,500 lost their lives, making the event a somber testament to the limits of human hubris and the vulnerabilities of technology.\n",
    "\n",
    "While the Titanic's story has been retold through numerous films, books, and documentaries, it also presents a unique data challenge. The Titanic dataset, which we'll delve into in this notebook, holds information about the passengers - from their age and gender to the fare they paid for their ticket. As aspiring data scientists in the field of Computational Social Science, our goal is to use this data not just to understand the tragedy better, but to also draw insights into socio-economic patterns of that era and, most importantly, predict survival based on various features.\n",
    "\n",
    "**Objectives of this Notebook**:\n",
    "\n",
    "1. **Understanding the Dataset**: Before any analysis, it's crucial to understand the nature and structure of our data. What information do we have? What might be missing?\n",
    "\n",
    "2. **Building a Baseline Model**: We'll dive into the world of supervised machine learning by predicting survival on the Titanic. But before we get sophisticated, let's see how well we can do with a simple, minimally processed dataset.\n",
    "\n",
    "3. **The Power of Feature Engineering**: After establishing a baseline, we'll explore the art and science of feature engineering. How can we extract or transform data to make it more valuable for our predictions?\n",
    "\n",
    "4. **Evaluation & Interpretation**: A model is only as good as its performance. We'll learn how to gauge the effectiveness of our predictions and interpret the results in the broader context of socio-economic indicators.\n",
    "\n",
    "5. **Broadening our Toolkit**: While we'll start with a logistic regression model, the world of machine learning is vast. We'll also get a glimpse of another model, the Decision Tree, and understand its strengths.\n",
    "\n",
    "By the end of this notebook, not only will you have a better appreciation for the intricacies of the Titanic dataset, but you'll also possess a foundational understanding of supervised machine learning, the importance of feature engineering, and the tools for model evaluation. So, let's embark on this analytical journey and unravel the patterns hidden in the Titanic's data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0945cf9221810e28d10a997e5061db56",
     "grade": false,
     "grade_id": "cell-d2c3e98fbc1cfb62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Setup and requirements\n",
    "\n",
    "Let's start by installing the required packages if you haven't done so already. Uncomment the following cell with command Ctrl+/ and run it to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "543b64e38798fb5c9c5ab067e3627c42",
     "grade": false,
     "grade_id": "cell-ac182722e45a7a80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9871ff3f3cbcda75709bf396b240a003",
     "grade": false,
     "grade_id": "cell-6236c81048d0ed2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Understanding the dataset\n",
    "\n",
    "### Loading the dataset\n",
    "\n",
    "Let's load the provided dataset with the data about the passengers into a pandas DataFrame and start exploring it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data paths\n",
    "training_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "\n",
    "# Read the data\n",
    "training_data = pd.read_csv(training_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "display(training_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec75714ebd7346fc0e676d9b82a13d0c",
     "grade": false,
     "grade_id": "cell-7d0252d32d599761",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Looking at the data\n",
    "\n",
    "Here we see the data that we are working with in a tabular format. It is important to get a good grasp of the data to see if their are any patterns or attributes that might turn out useful for us in predicting the survival of passengers. As a starter, let's take a look at all the columns our data has:\n",
    "\n",
    "- **PassengerId**: The unique ID of each passenger\n",
    "- **Survived**: Whether the passenger survived or not, 1 indicates the passanger survived; 0 if not.\n",
    "- **Pclass**: Ticket class \n",
    "- **Name**: The name of the passenger, including a title\n",
    "- **Sex**: Sex\n",
    "- **Age**: Age in years\n",
    "- **SibSp**: # of siblings / spouses aboard the Titanic\n",
    "- **Parch**: # of parents / children aboard the Titanic\n",
    "- **Ticket**: Ticket number\n",
    "- **Fare**: Passenger fare\n",
    "- **Cabin**: Cabin number\n",
    "- **Embarked**: Where did the passenger embark? S = Southampton, C = Cherbourg, \n",
    "\n",
    "Now that we have a general idea of what the data looks like, let's make some adjustments to make it easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70396524a64a777ff6d0ecf54fa7f0db",
     "grade": false,
     "grade_id": "cell-dbf045f20fc34b3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data Transformation\n",
    "As observed in the dataset, our variables consist of different datatypes. Features like `Survived` and `Pclass` are integers, whereas `Sex` and `Embarked` are strings. Machine learning models operate primarily on numerical data, requiring us to convert non-numerical values into a numerical format.\n",
    "\n",
    "Categorical data, such as Sex and Embarked, fall into two categories:\n",
    "\n",
    "- **Ordinal**: These have a natural order, like 'low', 'medium', 'high', or the `Pclass` in our dataset.\n",
    "- **Nominal**: These lack a natural order, like 'male' and 'female' or city names.\n",
    "For ordinal data, a simple integer encoding might suffice, representing each category with a unique integer. However, for nominal data, this can introduce unintentional relationships. For instance, encoding 'male' as 1 and 'female' as 2 might suggest a numeric order, confusing the model.\n",
    "\n",
    "This is where one-hot encoding comes into play. It transforms each nominal category into a new binary column. For Sex, we'd get two columns: is_Male and is_Female. An observation with 'male' would be represented as `is_Male` = 1 and `is_Female` = 0. However, in the case of this dataset, `is_Male` is the negation of `is_Female`, therefore we will only need one column to contain all information about sex.\n",
    "\n",
    "One-hot encoding ensures our machine learning models treat these values as distinct categories without imposing an artificial order, making our data suitable for training.\n",
    "\n",
    "### Assignment 1: Transform the data (4 pts)\n",
    "Now it is up to you to create one-hot encoding for the categorical data in our dataset. For this, we can use pandas built-in function `get_dummies`. Check out the documentation and complete the function below: https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "077f994e69329ecc08b96c59daa2f38e",
     "grade": false,
     "grade_id": "transforming_data",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transform_data(df):\n",
    "    \"\"\"\n",
    "    Transforms the input dataframe by encoding categorical variables in 4 ways:\n",
    "        1. Use pd.get_dummies to encode the 'Sex' column, set the 'drop_first' parameter to True.\n",
    "        2. Do the same for the 'Embarked column'\n",
    "        3. Concatenate the original dataframe with the newly generated columns.\n",
    "        4. Drop the original columns.\n",
    "        \n",
    "    Please do not rename any columns, \n",
    "    your solution will not pass the auto-graded tests if you do; \n",
    "    It's better if they do.\n",
    "    \n",
    "    Parameters:\n",
    "    - df : DataFrame : Input dataset\n",
    "    \n",
    "    Returns:\n",
    "    - df_transformed : DataFrame : Transformed dataset with one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "train_set = transform_data(training_data)\n",
    "\n",
    "display(train_set.head())\n",
    "display(test_set.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8146dac09ca5bcae076ba8ad67b6ed4c",
     "grade": true,
     "grade_id": "run_function",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "sample_data = pd.DataFrame({\n",
    "    'Sex': ['male', 'female', 'female', 'male'],\n",
    "    'Embarked': ['C', 'S', 'Q', 'S'],\n",
    "    'Age': [22, 28, 35, 54]\n",
    "})\n",
    "\n",
    "transformed_data = transform_data(sample_data.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3608d5bd5289dcec5f90789c294fb073",
     "grade": true,
     "grade_id": "columns_removed",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check if original columns are removed\n",
    "assert 'Sex' not in transformed_data.columns, \"The 'Sex' column should be removed.\"\n",
    "assert 'Embarked' not in transformed_data.columns, \"The 'Embarked' column should be removed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2514ec350a809d107debdac181dbd9ea",
     "grade": true,
     "grade_id": "one_hot_encoded",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check for one-hot encoded columns\n",
    "possible_sex_cols = sum([\"male\" in col.lower() or \"female\" in col.lower() for col in transformed_data.columns])\n",
    "assert possible_sex_cols >= 1, \"There should be at least one column related to 'Sex' encoding.\"\n",
    "\n",
    "possible_embarked_cols = sum([\"C\" in col or \"S\" in col or \"Q\" in col for col in transformed_data.columns])\n",
    "assert possible_embarked_cols >= 2, \"There should be at least two columns related to 'Embarked' encoding.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c0eefa0ea09db4030c29f5454e88614",
     "grade": true,
     "grade_id": "data_integrity",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check data integrity\n",
    "assert len(sample_data) == len(transformed_data), \"The number of rows should remain unchanged.\"\n",
    "male_encoded_values = sum([transformed_data[col].sum() for col in transformed_data.columns if \"male\" in col.lower()])\n",
    "assert male_encoded_values == 2, \"There should be two 'male' entries in the sample data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that our data is in a more numerical format we can look into any correlations that might help us in making predictions. Let's take a look at a correlation matrix. Pandas has a built-in function for this that calculates the Pearson correlation between every combination of two variables.\n",
    "\n",
    "The Pearson correlation coefficient, often denoted as $r$, is a measure that captures the strength and direction of a linear relationship between two variables. It returns a value between -1 and 1:\n",
    "\n",
    "- Close to 1: Indicates a strong positive linear relationship.\n",
    "- Close to -1: Indicates a strong negative linear relationship.\n",
    "- Close to 0: Indicates little to no linear relationship between the variables.\n",
    "\n",
    "Mathematical Definition:\n",
    "\n",
    "Given two variables, $X$ and $Y$, with data points $x_1, x_2, ... x_n$ and $y_1, y_2, ... y_n$ respectively, the Pearson correlation coefficient $r$ is defined as:\n",
    "\n",
    "$$r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\bar{x}$  is the mean of variable $X$.\n",
    "- $\\bar{y}$ is the mean of variable $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "273b1af2591288060e3c294b0a78f3a0",
     "grade": false,
     "grade_id": "cell-b868ecefaadfdd88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix with the designated columns, for now we disregard any non-numerical features.\n",
    "corr_matrix = train_set[['Survived', \n",
    "                         'Pclass', \n",
    "                         'Age', \n",
    "                         'SibSp', \n",
    "                         'Parch', \n",
    "                         'Fare', \n",
    "                         'is_male', \n",
    "                         'embarked_C', \n",
    "                         'embarked_Q', \n",
    "                         'embarked_S']].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Matrix of Transformed Titanic Dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89a79e1486e0fe4e60dea0c7989a98b8",
     "grade": false,
     "grade_id": "cell-e2de08d27868642f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 2: Plot interpretation (4 pts)\n",
    "Answer the following questions about the plot in the cell below:\n",
    "\n",
    "1. Why are values along the diagonal $1$?\n",
    "2. Which two distinct features show the strongest correlation?\n",
    "3. Had we still included the `is_female` column, what would be the Pearson coefficient with `is_male`?\n",
    "4. Which single feature is the most important in predicting our target label?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17baef6c53706e8d4e2929b82f0007ee",
     "grade": true,
     "grade_id": "cell-de53375559653cd0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3: Creating our first model (6 pts)\n",
    "\n",
    "-- Here there will be some basic explanation about decision trees.\n",
    "\n",
    "-- Maybe ask if they could make a decision tree with on decision what would it be to check intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "168c7bb136cbd05c8606980396d332e1",
     "grade": false,
     "grade_id": "cell-1d341a3442ff69a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data into training and test set, for your features, make sure you exclude the target class and any non-numerical features.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def train_decision_tree(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train a Decision Tree classifier on the provided training data and labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train : DataFrame : Training features\n",
    "    - y_train : Series : Training labels\n",
    "    \n",
    "    Returns:\n",
    "    - clf : DecisionTreeClassifier : Trained Decision Tree classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return clf\n",
    "\n",
    "# Autograding tests\n",
    "clf = train_decision_tree(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79493c10d20140cdacb9906cf07f9220",
     "grade": true,
     "grade_id": "cell-f99662ce2bdef946",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(clf, DecisionTreeClassifier)\n",
    "assert 'Survived' not in X.columns\n",
    "assert accuracy_score(y_test, y_pred) > 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ecb5adb148a73de86583d6af1ef260b5",
     "grade": false,
     "grade_id": "cell-1f97071610ce3d1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To be continued with... random forests? Some feature engineering?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
